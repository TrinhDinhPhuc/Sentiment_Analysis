{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on Trump's tweets using Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <strong>NumPy</strong>: This is the fundamental package for scientific computing with Python. Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data.\n",
    "- <strong> Pandas</strong>: This is an open source library providing high-performance, easy-to-use data structures and data analysis tools.\n",
    "- <strong>Tweepy</strong>: This is an easy-to-use Python library for accessing the Twitter API.\n",
    "- <strong>Matplotlib</strong>: This is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "- <strong>Seaborn</strong>: This is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.\n",
    "- <strong>Textblob</strong>: This is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extracting twitter data (tweepy + pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General:\n",
    "import tweepy           # To consume Twitter's API\n",
    "import pandas as pd     # To handle data\n",
    "import numpy as np      # For number computing\n",
    "\n",
    "# For plotting and visualization:\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import our access keys:\n",
    "from credentials import * # This will allow us to use the keys as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Consume:\n",
    "# CONSUMER_KEY    = 'CrMSxqJz6zvwEa85OOKdUpSeH'\n",
    "# CONSUMER_SECRET = 'hkdYe8u9SB30Y67uY9RdqRwrh00gZyFedYHI4iyFziRDeVjPxx'\n",
    " \n",
    "# # Access:\n",
    "# ACCESS_TOKEN  = '3093984751-BpI2Oat6hRf8qEWuIfKlz3Hrn8vsPm3KjOn6GvF'\n",
    "# ACCESS_SECRET = 'tISJPGvRQ2TZJhKuNA0HZ9kORuS699gsnsVzoh7YV8rUU' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API's setup:\n",
    "def twitter_setup():\n",
    "    # Authentication and access using keys:\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    # Return API with authentication:\n",
    "    api = tweepy.API(auth)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a function to setup the Twitter API, we can use this function to create an \"extractor\" object. After this, we will use Tweepy's function extractor.user_timeline(screen_name, count) to extract from screen_name's user the quantity of count tweets.\n",
    "\n",
    "As it is mentioned in the title, I've chosen @realDonaldTrump as the user to extract data for a posterior analysis. Yeah, we wanna keep it interesting, LOL.\n",
    "\n",
    "The way to extract Twitter's data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets extracted: 200.\n",
      "\n",
      "5 recent tweets:\n",
      "\n",
      "I will be strongly pushing Comprehensive Background Checks with an emphasis on Mental Health. Raise age to 21 and e… https://t.co/5zKlGxnVTc\n",
      "\n",
      "....If a potential “sicko shooter” knows that a school has a large number of very weapons talented teachers (and ot… https://t.co/NTnUcZxrle\n",
      "\n",
      "....History shows that a school shooting lasts, on average, 3 minutes. It takes police &amp; first responders approxima… https://t.co/v68k63he25\n",
      "\n",
      "....immediately fire back if a savage sicko came to a school with bad intentions. Highly trained teachers would als… https://t.co/wyg77QWPfn\n",
      "\n",
      "I never said “give teachers guns” like was stated on Fake News @CNN &amp; @NBC. What I said was to look at the possibil… https://t.co/Tvi4fzLbCA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We create an extractor object:\n",
    "extractor = twitter_setup()\n",
    "\n",
    "# We create a tweet list as follows:\n",
    "tweets = extractor.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "print(\"Number of tweets extracted: {}.\\n\".format(len(tweets)))\n",
    "\n",
    "# We print the most recent 5 tweets:\n",
    "print(\"5 recent tweets:\\n\")\n",
    "for tweet in tweets[:5]:\n",
    "    print(tweet.text)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a (pandas) DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n",
    "\n",
    "# We display the first 10 elements of the dataframe:\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Internal methods of a single tweet object:\n",
    "print(dir(tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We print info from the first tweet:\n",
    "print(tweets[0].id)\n",
    "print(tweets[0].created_at)\n",
    "print(tweets[0].source)\n",
    "print(tweets[0].favorite_count)\n",
    "print(tweets[0].retweet_count)\n",
    "print(tweets[0].geo)\n",
    "print(tweets[0].coordinates)\n",
    "print(tweets[0].entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Adding relevant info to our dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can obtain a lot of data from a single tweet. But not all this data is always useful for specific stuff. In our case we well just add some data to our dataframe. For this we will use Pythons list comprehension and a new column will be added to the dataframe by just simply adding the name of the content between square brackets and assign the content. The code goes as...:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We add relevant data:\n",
    "data['len']  = np.array([len(tweet.text) for tweet in tweets])\n",
    "data['ID']   = np.array([tweet.id for tweet in tweets])\n",
    "data['Date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "data['Source'] = np.array([tweet.source for tweet in tweets])\n",
    "data['Likes']  = np.array([tweet.favorite_count for tweet in tweets])\n",
    "data['RTs']    = np.array([tweet.retweet_count for tweet in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display of first 10 elements from dataframe:\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualization and basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Averages and popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We extract the mean of lenghts:\n",
    "mean = np.mean(data['len'])\n",
    "\n",
    "print(\"The lenght's average in tweets: {}\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We extract the tweet with more FAVs and more RTs:\n",
    "\n",
    "fav_max = np.max(data['Likes'])\n",
    "rt_max  = np.max(data['RTs'])\n",
    "\n",
    "fav = data[data.Likes == fav_max].index[0]\n",
    "rt  = data[data.RTs == rt_max].index[0]\n",
    "\n",
    "# Max FAVs:\n",
    "print(\"The tweet with more likes is: \\n{}\".format(data['Tweets'][fav]))\n",
    "print(\"Number of likes: {}\".format(fav_max))\n",
    "print(\"{} characters.\\n\".format(data['len'][fav]))\n",
    "\n",
    "# Max RTs:\n",
    "print(\"The tweet with more retweets is: \\n{}\".format(data['Tweets'][rt]))\n",
    "print(\"Number of retweets: {}\".format(rt_max))\n",
    "print(\"{} characters.\\n\".format(data['len'][rt]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has its own object for time series. Since we have a whole vector with creation dates, we can construct time series respect tweets lengths, likes and retweets.\n",
    "\n",
    "The way we do it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create time series for data:\n",
    "tlen = pd.Series(data=data['len'].values, index=data['Date'])\n",
    "tfav = pd.Series(data=data['Likes'].values, index=data['Date'])\n",
    "tret = pd.Series(data=data['RTs'].values, index=data['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to plot the time series, pandas already has its own method in the object. We can plot a time series as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lenghts along time:\n",
    "tlen.plot(figsize=(16,4), color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to plot the likes versus the retweets in the same chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Likes vs retweets visualization:\n",
    "\n",
    "tfav.plot(figsize=(16,4), label=\"Likes\", legend=True)\n",
    "tret.plot(figsize=(16,4), label=\"Retweets\", legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3. Pie charts of sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done with this second section of the post. Now we will plot the sources in a pie chart, since we realized that not every tweet is tweeted from the same source (😱🤔). We first clean all the sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We obtain all possible sources:\n",
    "sources = []\n",
    "for source in data['Source']:\n",
    "    if source not in sources:\n",
    "        sources.append(source)\n",
    "\n",
    "# We print sources list:\n",
    "print(\"Creation of content sources:\")\n",
    "for source in sources:\n",
    "    print(\"* {}\".format(source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now count the number of each source and create a pie chart. You'll notice that this code cell is not the most optimized one... Please have in mind that it was 4 in the morning when I was designing this workshop. 😅\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We create a numpy vector mapped to labels:\n",
    "percent = np.zeros(len(sources))\n",
    "\n",
    "for source in data['Source']:\n",
    "    for index in range(len(sources)):\n",
    "        if source == sources[index]:\n",
    "            percent[index] += 1\n",
    "            pass\n",
    "\n",
    "percent /= 100\n",
    "\n",
    "# Pie chart:\n",
    "pie_chart = pd.Series(percent, index=sources, name='Sources')\n",
    "pie_chart.plot.pie(fontsize=11, autopct='%.2f', figsize=(6, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Importing textblob\n",
    "\n",
    "As we mentioned at the beginning of this post, textblob will allow us to do sentiment analysis in a very simple way. We will also use the re library from Python, which is used to work with regular expressions. For this, I'll provide you two utility functions to: a) clean text (which means that any symbol distinct to an alphanumeric value will be remapped into a new one that satisfies this condition), and b) create a classifier to analyze the polarity of each tweet after cleaning the text in it. I won't explain the specific way in which the function that cleans works, since it would be extended and it might be better understood in the official redocumentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def analize_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way it works is that textblob already provides a trained analyzer (cool, right?). Textblob can work with different machine learning models used in natural language processing. If you want to train your own classifier (or at least check how it works) feel free to check the following link (https://textblob.readthedocs.io/en/dev/classifiers.html). It might result relevant since we're working with a pre-trained model (for which we don't not the data that was used).\n",
    "\n",
    "Anyway, getting back to the code we will just add an extra column to our data. This column will contain the sentiment analysis and we can plot the dataframe to see the update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We create a column with the result of the analysis:\n",
    "data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])\n",
    "\n",
    "# We display the updated dataframe with the new column:\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Analyzing the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a simple way to verify the results, we will count the number of neutral, positive and negative tweets and extract the percentages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We construct lists with classified tweets:\n",
    "pos_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] > 0]\n",
    "neu_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] == 0]\n",
    "neg_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the lists, we just print the percentages:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We print percentages:\n",
    "\n",
    "print(\"Percentage of positive tweets: {}%\".format(len(pos_tweets)*100/len(data['Tweets'])))\n",
    "print(\"Percentage of neutral tweets: {}%\".format(len(neu_tweets)*100/len(data['Tweets'])))\n",
    "print(\"Percentage de negative tweets: {}%\".format(len(neg_tweets)*100/len(data['Tweets'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive = pos_tweets*100/len(data['Tweets'])\n",
    "negative = neu_tweets*100/len(data['Tweets']\n",
    "neutral =  neg_tweets*100/len(data['Tweets'] \n",
    "colors = ['green', 'red', 'grey']\n",
    "sizes = [positive, negative, neutral]\n",
    "labels = 'Positive', 'Negative', 'Neutral'\n",
    "\n",
    "## use matplotlib to plot the chart\n",
    "plt.pie(\n",
    "   x=sizes,\n",
    "   shadow=True,\n",
    "   colors=colors,\n",
    "   labels=labels,\n",
    "   startangle=90\n",
    ")\n",
    "\n",
    "plt.title(\"Sentiment of {} Tweets about {}\".format(number, query))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv('TrumpsTweet.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
